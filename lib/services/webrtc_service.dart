import 'dart:async';
import 'package:flutter_webrtc/flutter_webrtc.dart';
import 'websocket_service.dart';
import '../utils/logger.dart';

enum CallState {
  idle, // ç©ºé—²
  calling, // æ­£åœ¨å‘¼å«
  ringing, // å¯¹æ–¹æ¥ç”µå“é“ƒä¸­
  connected, // å·²è¿æ¥
  ended, // å·²ç»“æŸ
}

enum CallType {
  voice, // è¯­éŸ³é€šè¯
  video, // è§†é¢‘é€šè¯
}

class WebRTCService {
  // å•ä¾‹æ¨¡å¼
  static final WebRTCService _instance = WebRTCService._internal();
  factory WebRTCService() => _instance;
  WebRTCService._internal();

  // WebRTC ç›¸å…³
  RTCPeerConnection? _peerConnection;
  MediaStream? _localStream;
  MediaStream? _remoteStream;

  // é€šè¯çŠ¶æ€
  CallState _callState = CallState.idle;
  CallType _callType = CallType.voice;
  int? _currentCallUserId; // å½“å‰é€šè¯çš„å¯¹æ–¹ç”¨æˆ·ID

  // WebSocket æœåŠ¡
  final WebSocketService _wsService = WebSocketService();

  // å›è°ƒå‡½æ•°
  Function(CallState)? onCallStateChanged;
  Function(MediaStream)? onRemoteStreamAdded;
  Function(MediaStream)? onLocalStreamAdded;
  Function(String)? onError;
  Function(int userId, String displayName, CallType callType)? onIncomingCall;

  // STUN/TURN æœåŠ¡å™¨é…ç½®
  final Map<String, dynamic> _configuration = {
    'iceServers': [
      {
        // Google å…¬å…± STUN æœåŠ¡å™¨
        'urls': [
          'stun:stun.l.google.com:19302',
          'stun:stun1.l.google.com:19302',
        ],
      },
      {
        // æ‚¨çš„ TURN æœåŠ¡å™¨ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…çš„æœåŠ¡å™¨åœ°å€å’Œå‡­è¯ï¼‰
        'urls': [
          'turn:31.57.65.81:3478?transport=udp',
          'turn:31.57.65.81:3478?transport=tcp',
        ],
        'username': 'youdu-turn',
        'credential': "D@S&#D>!c3dqd",
      },
    ],
    'sdpSemantics': 'unified-plan',
  };

  // Offer/Answer çº¦æŸ
  final Map<String, dynamic> _offerSdpConstraints = {
    'mandatory': {'OfferToReceiveAudio': true, 'OfferToReceiveVideo': true},
    'optional': [],
  };

  // åˆå§‹åŒ–
  Future<void> initialize(int currentUserId) async {
    _setupWebSocketListeners();
    logger.debug('ğŸ“ WebRTC æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼Œç”¨æˆ·ID: $currentUserId');
  }

  // è®¾ç½® WebSocket ç›‘å¬å™¨
  void _setupWebSocketListeners() {
    _wsService.onWebRTCSignal = (data) async {
      logger.debug('ğŸ“ æ”¶åˆ° WebRTC ä¿¡ä»¤: ${data['type']}');

      try {
        switch (data['type']) {
          case 'offer':
            await _handleOffer(data);
            break;
          case 'answer':
            await _handleAnswer(data);
            break;
          case 'ice-candidate':
            await _handleIceCandidate(data);
            break;
          case 'call-request':
            _handleIncomingCall(data);
            break;
          case 'call-accepted':
            await _handleCallAccepted(data);
            break;
          case 'call-rejected':
            _handleCallRejected(data);
            break;
          case 'call-ended':
            await endCall();
            break;
        }
      } catch (e) {
        logger.debug('âŒ å¤„ç† WebRTC ä¿¡ä»¤å¤±è´¥: $e');
        onError?.call('ä¿¡ä»¤å¤„ç†å¤±è´¥: $e');
      }
    };
  }

  // å‘èµ·è¯­éŸ³é€šè¯
  Future<void> startVoiceCall(
    int targetUserId,
    String targetDisplayName,
  ) async {
    await _startCall(targetUserId, targetDisplayName, CallType.voice);
  }

  // å‘èµ·è§†é¢‘é€šè¯
  Future<void> startVideoCall(
    int targetUserId,
    String targetDisplayName,
  ) async {
    await _startCall(targetUserId, targetDisplayName, CallType.video);
  }

  // å‘èµ·é€šè¯
  Future<void> _startCall(
    int targetUserId,
    String targetDisplayName,
    CallType callType,
  ) async {
    if (_callState != CallState.idle) {
      onError?.call('å½“å‰æ­£åœ¨é€šè¯ä¸­');
      return;
    }

    try {
      logger.debug(
        'ğŸ“ å‘èµ·${callType == CallType.voice ? 'è¯­éŸ³' : 'è§†é¢‘'}é€šè¯ï¼Œç›®æ ‡ç”¨æˆ·: $targetUserId',
      );

      _currentCallUserId = targetUserId;
      _callType = callType;
      _updateCallState(CallState.calling);

      // åˆ›å»ºæœ¬åœ°åª’ä½“æµ
      await _createLocalStream(callType);

      // åˆ›å»º PeerConnection
      await _createPeerConnection();

      // å‘é€é€šè¯è¯·æ±‚
      _wsService.sendWebRTCSignal({
        'type': 'call-request',
        'targetUserId': targetUserId,
        'callType': callType == CallType.voice ? 'voice' : 'video',
        'callerName': targetDisplayName,
      });

      logger.debug('ğŸ“ é€šè¯è¯·æ±‚å·²å‘é€');
    } catch (e) {
      logger.debug('âŒ å‘èµ·é€šè¯å¤±è´¥: $e');
      onError?.call('å‘èµ·é€šè¯å¤±è´¥: $e');
      await endCall();
    }
  }

  // æ¥å¬æ¥ç”µ
  Future<void> acceptCall() async {
    if (_callState != CallState.ringing) {
      logger.debug('âŒ å½“å‰æ²¡æœ‰æ¥ç”µ');
      return;
    }

    try {
      logger.debug('ğŸ“ æ¥å¬æ¥ç”µ');

      // åˆ›å»ºæœ¬åœ°åª’ä½“æµ
      await _createLocalStream(_callType);

      // åˆ›å»º PeerConnection
      await _createPeerConnection();

      // å¯ç”¨æ‰¬å£°å™¨ï¼ˆè¯­éŸ³é€šè¯å¿…é¡»ï¼‰
      try {
        await Helper.setSpeakerphoneOn(true);
        logger.debug('ğŸ“ æ‰¬å£°å™¨å·²å¯ç”¨');
      } catch (e) {
        logger.debug('âš ï¸ å¯ç”¨æ‰¬å£°å™¨å¤±è´¥: $e');
      }

      // é€šçŸ¥å¯¹æ–¹å·²æ¥å—é€šè¯
      _wsService.sendWebRTCSignal({
        'type': 'call-accepted',
        'targetUserId': _currentCallUserId,
      });

      _updateCallState(CallState.connected);
    } catch (e) {
      logger.debug('âŒ æ¥å¬æ¥ç”µå¤±è´¥: $e');
      onError?.call('æ¥å¬æ¥ç”µå¤±è´¥: $e');
      await endCall();
    }
  }

  // æ‹’ç»æ¥ç”µ
  Future<void> rejectCall() async {
    if (_callState != CallState.ringing) {
      logger.debug('âŒ å½“å‰æ²¡æœ‰æ¥ç”µ');
      return;
    }

    logger.debug('ğŸ“ æ‹’ç»æ¥ç”µ');

    _wsService.sendWebRTCSignal({
      'type': 'call-rejected',
      'targetUserId': _currentCallUserId,
    });

    await endCall();
  }

  // ç»“æŸé€šè¯
  Future<void> endCall() async {
    // é˜²æ­¢é‡å¤è°ƒç”¨
    if (_callState == CallState.idle || _callState == CallState.ended) {
      logger.debug('ğŸ“ é€šè¯å·²ç»“æŸï¼Œè·³è¿‡é‡å¤è°ƒç”¨');
      return;
    }

    logger.debug('ğŸ“ ç»“æŸé€šè¯');

    // é€šçŸ¥å¯¹æ–¹æŒ‚æ–­ï¼ˆåªåœ¨ä¸»åŠ¨æŒ‚æ–­æ—¶å‘é€ï¼‰
    if (_currentCallUserId != null && _callState != CallState.idle) {
      try {
        _wsService.sendWebRTCSignal({
          'type': 'call-ended',
          'targetUserId': _currentCallUserId,
        });
      } catch (e) {
        logger.debug('âš ï¸ å‘é€æŒ‚æ–­ä¿¡ä»¤å¤±è´¥: $e');
      }
    }

    // å…ˆå…³é—­ PeerConnectionï¼ˆè¿™ä¼šè‡ªåŠ¨åœæ­¢trackï¼‰
    if (_peerConnection != null) {
      try {
        await _peerConnection!.close();
        logger.debug('ğŸ“ PeerConnection å·²å…³é—­');
      } catch (e) {
        logger.debug('âš ï¸ å…³é—­ PeerConnection å¤±è´¥: $e');
      }
      _peerConnection = null;
    }

    // åœæ­¢å¹¶é‡Šæ”¾æœ¬åœ°æµ
    if (_localStream != null) {
      try {
        // å…ˆåœæ­¢æ‰€æœ‰track
        _localStream!.getTracks().forEach((track) {
          track.stop();
        });
        // å†disposeæµ
        await _localStream!.dispose();
        logger.debug('ğŸ“ æœ¬åœ°æµå·²é‡Šæ”¾');
      } catch (e) {
        logger.debug('âš ï¸ é‡Šæ”¾æœ¬åœ°æµå¤±è´¥: $e');
      }
      _localStream = null;
    }

    // åœæ­¢å¹¶é‡Šæ”¾è¿œç¨‹æµ
    if (_remoteStream != null) {
      try {
        // å…ˆåœæ­¢æ‰€æœ‰track
        _remoteStream!.getTracks().forEach((track) {
          track.stop();
        });
        // å†disposeæµ
        await _remoteStream!.dispose();
        logger.debug('ğŸ“ è¿œç¨‹æµå·²é‡Šæ”¾');
      } catch (e) {
        logger.debug('âš ï¸ é‡Šæ”¾è¿œç¨‹æµå¤±è´¥: $e');
      }
      _remoteStream = null;
    }

    _currentCallUserId = null;
    _updateCallState(CallState.ended);

    // å»¶è¿Ÿé‡ç½®ä¸ºidleçŠ¶æ€
    await Future.delayed(const Duration(milliseconds: 500));
    _updateCallState(CallState.idle);
  }

  // åˆ›å»ºæœ¬åœ°åª’ä½“æµ
  Future<void> _createLocalStream(CallType callType) async {
    logger.debug('ğŸ“ åˆ›å»ºæœ¬åœ°åª’ä½“æµ: ${callType == CallType.voice ? 'ä»…éŸ³é¢‘' : 'éŸ³è§†é¢‘'}');

    final Map<String, dynamic> mediaConstraints = {
      'audio': {
        'mandatory': {
          'googEchoCancellation': true,
          'googAutoGainControl': true,
          'googNoiseSuppression': true,
          'googHighpassFilter': true,
        },
        'optional': [],
      },
      'video': callType == CallType.video
          ? {
              'mandatory': {
                'minWidth': 640,
                'minHeight': 480,
                'minFrameRate': 30,
              },
              'facingMode': 'user',
              'optional': [],
            }
          : false,
    };

    try {
      _localStream = await navigator.mediaDevices.getUserMedia(
        mediaConstraints,
      );

      // æ‰“å°å¹¶ç¡®ä¿éŸ³é¢‘è½¨é“å·²å¯ç”¨
      final audioTracks = _localStream!.getAudioTracks();
      logger.debug('ğŸ”Š æœ¬åœ°éŸ³é¢‘è½¨é“æ•°é‡: ${audioTracks.length}');
      for (var track in audioTracks) {
        track.enabled = true;
        logger.debug(
          'ğŸ”Š æœ¬åœ°éŸ³é¢‘è½¨é“ ${track.id}: enabled=${track.enabled}, kind=${track.kind}, muted=${track.muted}',
        );
      }

      // æ‰“å°è§†é¢‘è½¨é“ä¿¡æ¯
      if (callType == CallType.video) {
        final videoTracks = _localStream!.getVideoTracks();
        logger.debug('ğŸ“¹ è§†é¢‘è½¨é“æ•°é‡: ${videoTracks.length}');
        if (videoTracks.isNotEmpty) {
          logger.debug('ğŸ“¹ è§†é¢‘è½¨é“ID: ${videoTracks[0].id}');
          logger.debug('ğŸ“¹ è§†é¢‘è½¨é“å¯ç”¨: ${videoTracks[0].enabled}');
          logger.debug('ğŸ“¹ è§†é¢‘è½¨é“ç§ç±»: ${videoTracks[0].kind}');
        }
      }

      onLocalStreamAdded?.call(_localStream!);
      logger.debug('ğŸ“ æœ¬åœ°åª’ä½“æµåˆ›å»ºæˆåŠŸ');
    } catch (e) {
      logger.debug('âŒ åˆ›å»ºæœ¬åœ°åª’ä½“æµå¤±è´¥: $e');
      throw Exception('æ— æ³•è®¿é—®éº¦å…‹é£/æ‘„åƒå¤´: $e');
    }
  }

  // åˆ›å»º PeerConnection
  Future<void> _createPeerConnection() async {
    logger.debug('ğŸ“ åˆ›å»º PeerConnection');

    _peerConnection = await createPeerConnection(_configuration);

    // æ·»åŠ æœ¬åœ°æµåˆ° PeerConnection
    if (_localStream != null) {
      _localStream!.getTracks().forEach((track) {
        _peerConnection!.addTrack(track, _localStream!);
      });
    }

    // ç›‘å¬è¿œç¨‹æµ
    _peerConnection!.onTrack = (RTCTrackEvent event) {
      logger.debug('ğŸ“ æ”¶åˆ°è¿œç¨‹æµè½¨é“: ${event.track.kind}');
      logger.debug(
        'ğŸ“ è½¨é“è¯¦æƒ…: id=${event.track.id}, enabled=${event.track.enabled}, muted=${event.track.muted}',
      );

      if (event.streams.isNotEmpty) {
        _remoteStream = event.streams[0];
        logger.debug('ğŸ“ è¿œç¨‹æµID: ${_remoteStream!.id}');

        // ç¡®ä¿éŸ³é¢‘è½¨é“å·²å¯ç”¨
        final audioTracks = _remoteStream!.getAudioTracks();
        logger.debug('ğŸ“ è¿œç¨‹éŸ³é¢‘è½¨é“æ•°: ${audioTracks.length}');
        for (var track in audioTracks) {
          // å¼ºåˆ¶å¯ç”¨éŸ³é¢‘è½¨é“
          track.enabled = true;
          logger.debug(
            'ğŸ“ è¿œç¨‹éŸ³é¢‘è½¨é“ ${track.id}: enabled=${track.enabled}, muted=${track.muted}',
          );
        }

        // æ£€æŸ¥è§†é¢‘è½¨é“ï¼ˆå¦‚æœæœ‰ï¼‰
        final videoTracks = _remoteStream!.getVideoTracks();
        logger.debug('ğŸ“ è¿œç¨‹è§†é¢‘è½¨é“æ•°: ${videoTracks.length}');

        onRemoteStreamAdded?.call(_remoteStream!);
      } else {
        logger.debug('âš ï¸ è­¦å‘Š: æ”¶åˆ°è½¨é“ä½†æ²¡æœ‰æµ');
      }
    };

    // ç›‘å¬ ICE å€™é€‰
    _peerConnection!.onIceCandidate = (RTCIceCandidate candidate) {
      logger.debug('ğŸ“ å‘é€ ICE å€™é€‰');
      _wsService.sendWebRTCSignal({
        'type': 'ice-candidate',
        'targetUserId': _currentCallUserId,
        'candidate': {
          'candidate': candidate.candidate,
          'sdpMid': candidate.sdpMid,
          'sdpMLineIndex': candidate.sdpMLineIndex,
        },
      });
    };

    // ç›‘å¬è¿æ¥çŠ¶æ€å˜åŒ–
    _peerConnection!.onConnectionState = (RTCPeerConnectionState state) {
      logger.debug('ğŸ“ è¿æ¥çŠ¶æ€å˜åŒ–: $state');

      // å½“è¿æ¥æˆåŠŸæ—¶ï¼Œå†æ¬¡ç¡®è®¤éŸ³é¢‘è½¨é“å¯ç”¨
      if (state == RTCPeerConnectionState.RTCPeerConnectionStateConnected) {
        logger.debug('âœ… WebRTC è¿æ¥å·²å»ºç«‹');

        // æ£€æŸ¥æœ¬åœ°éŸ³é¢‘è½¨é“
        if (_localStream != null) {
          final localAudioTracks = _localStream!.getAudioTracks();
          logger.debug('ğŸ”Š è¿æ¥åæœ¬åœ°éŸ³é¢‘è½¨é“æ•°: ${localAudioTracks.length}');
          for (var track in localAudioTracks) {
            logger.debug(
              'ğŸ”Š æœ¬åœ°éŸ³é¢‘è½¨é“ ${track.id}: enabled=${track.enabled}, muted=${track.muted}',
            );
          }
        }

        // æ£€æŸ¥è¿œç¨‹éŸ³é¢‘è½¨é“
        if (_remoteStream != null) {
          final remoteAudioTracks = _remoteStream!.getAudioTracks();
          logger.debug('ğŸ”Š è¿æ¥åè¿œç¨‹éŸ³é¢‘è½¨é“æ•°: ${remoteAudioTracks.length}');
          for (var track in remoteAudioTracks) {
            logger.debug(
              'ğŸ”Š è¿œç¨‹éŸ³é¢‘è½¨é“ ${track.id}: enabled=${track.enabled}, muted=${track.muted}',
            );
          }
        }
      }

      if (state == RTCPeerConnectionState.RTCPeerConnectionStateFailed ||
          state == RTCPeerConnectionState.RTCPeerConnectionStateDisconnected ||
          state == RTCPeerConnectionState.RTCPeerConnectionStateClosed) {
        endCall();
      }
    };

    logger.debug('ğŸ“ PeerConnection åˆ›å»ºæˆåŠŸ');
  }

  // å¤„ç†æ¥ç”µ
  void _handleIncomingCall(Map<String, dynamic> data) {
    logger.debug('ğŸ“ æ”¶åˆ°æ¥ç”µ: $data');

    _currentCallUserId = data['fromUserId'];
    _callType = data['callType'] == 'video' ? CallType.video : CallType.voice;
    _updateCallState(CallState.ringing);

    onIncomingCall?.call(
      _currentCallUserId!,
      data['callerName'] ?? 'æœªçŸ¥ç”¨æˆ·',
      _callType,
    );
  }

  // å¤„ç†å¯¹æ–¹æ¥å—é€šè¯
  Future<void> _handleCallAccepted(Map<String, dynamic> data) async {
    logger.debug('ğŸ“ å¯¹æ–¹å·²æ¥å—é€šè¯');
    _updateCallState(CallState.connected);

    // å¯ç”¨æ‰¬å£°å™¨ï¼ˆè¯­éŸ³é€šè¯å¿…é¡»ï¼‰
    try {
      await Helper.setSpeakerphoneOn(true);
      logger.debug('ğŸ“ æ‰¬å£°å™¨å·²å¯ç”¨');
    } catch (e) {
      logger.debug('âš ï¸ å¯ç”¨æ‰¬å£°å™¨å¤±è´¥: $e');
    }

    // åˆ›å»ºå¹¶å‘é€ Offer
    try {
      RTCSessionDescription offer = await _peerConnection!.createOffer(
        _offerSdpConstraints,
      );
      await _peerConnection!.setLocalDescription(offer);

      _wsService.sendWebRTCSignal({
        'type': 'offer',
        'targetUserId': _currentCallUserId,
        'sdp': offer.sdp,
      });

      logger.debug('ğŸ“ Offer å·²å‘é€');
    } catch (e) {
      logger.debug('âŒ åˆ›å»º Offer å¤±è´¥: $e');
      onError?.call('å»ºç«‹è¿æ¥å¤±è´¥: $e');
      await endCall();
    }
  }

  // å¤„ç†å¯¹æ–¹æ‹’ç»é€šè¯
  void _handleCallRejected(Map<String, dynamic> data) {
    logger.debug('ğŸ“ å¯¹æ–¹æ‹’ç»äº†é€šè¯');
    onError?.call('å¯¹æ–¹æ‹’ç»äº†é€šè¯');
    endCall();
  }

  // å¤„ç† Offer
  Future<void> _handleOffer(Map<String, dynamic> data) async {
    logger.debug('ğŸ“ æ”¶åˆ° Offer');

    try {
      RTCSessionDescription description = RTCSessionDescription(
        data['sdp'],
        'offer',
      );

      await _peerConnection!.setRemoteDescription(description);

      // åˆ›å»ºå¹¶å‘é€ Answer
      RTCSessionDescription answer = await _peerConnection!.createAnswer(
        _offerSdpConstraints,
      );
      await _peerConnection!.setLocalDescription(answer);

      _wsService.sendWebRTCSignal({
        'type': 'answer',
        'targetUserId': _currentCallUserId,
        'sdp': answer.sdp,
      });

      logger.debug('ğŸ“ Answer å·²å‘é€');
    } catch (e) {
      logger.debug('âŒ å¤„ç† Offer å¤±è´¥: $e');
      onError?.call('å»ºç«‹è¿æ¥å¤±è´¥: $e');
      await endCall();
    }
  }

  // å¤„ç† Answer
  Future<void> _handleAnswer(Map<String, dynamic> data) async {
    logger.debug('ğŸ“ æ”¶åˆ° Answer');

    try {
      RTCSessionDescription description = RTCSessionDescription(
        data['sdp'],
        'answer',
      );

      await _peerConnection!.setRemoteDescription(description);
      logger.debug('ğŸ“ Answer è®¾ç½®æˆåŠŸ');
    } catch (e) {
      logger.debug('âŒ å¤„ç† Answer å¤±è´¥: $e');
      onError?.call('å»ºç«‹è¿æ¥å¤±è´¥: $e');
      await endCall();
    }
  }

  // å¤„ç† ICE å€™é€‰
  Future<void> _handleIceCandidate(Map<String, dynamic> data) async {
    logger.debug('ğŸ“ æ”¶åˆ° ICE å€™é€‰');

    try {
      final candidateData = data['candidate'];
      RTCIceCandidate candidate = RTCIceCandidate(
        candidateData['candidate'],
        candidateData['sdpMid'],
        candidateData['sdpMLineIndex'],
      );

      await _peerConnection!.addCandidate(candidate);
      logger.debug('ğŸ“ ICE å€™é€‰æ·»åŠ æˆåŠŸ');
    } catch (e) {
      logger.debug('âŒ æ·»åŠ  ICE å€™é€‰å¤±è´¥: $e');
    }
  }

  // æ›´æ–°é€šè¯çŠ¶æ€
  void _updateCallState(CallState newState) {
    _callState = newState;
    onCallStateChanged?.call(newState);
    logger.debug('ğŸ“ é€šè¯çŠ¶æ€å˜åŒ–: $newState');
  }

  // åˆ‡æ¢é™éŸ³
  void toggleMute() {
    if (_localStream != null) {
      final audioTracks = _localStream!.getAudioTracks();
      if (audioTracks.isNotEmpty) {
        final bool enabled = audioTracks[0].enabled;
        audioTracks[0].enabled = !enabled;
        logger.debug('ğŸ“ ${enabled ? 'é™éŸ³' : 'å–æ¶ˆé™éŸ³'}');
      }
    }
  }

  // åˆ‡æ¢æ‰¬å£°å™¨
  Future<void> toggleSpeaker() async {
    if (_localStream != null) {
      final audioTracks = _localStream!.getAudioTracks();
      if (audioTracks.isNotEmpty) {
        // è¿™é‡Œå¯ä»¥æ·»åŠ æ‰¬å£°å™¨åˆ‡æ¢é€»è¾‘
        await Helper.setSpeakerphoneOn(true);
        logger.debug('ğŸ“ æ‰¬å£°å™¨å·²å¼€å¯');
      }
    }
  }

  // è·å–å½“å‰çŠ¶æ€
  CallState get callState => _callState;
  CallType get callType => _callType;
  int? get currentCallUserId => _currentCallUserId;
  MediaStream? get localStream => _localStream;
  MediaStream? get remoteStream => _remoteStream;

  // æ¸…ç†èµ„æº
  Future<void> dispose() async {
    await endCall();
    _wsService.onWebRTCSignal = null;
  }
}
